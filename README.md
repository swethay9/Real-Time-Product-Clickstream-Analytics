# Real-Time Product Clickstream Analytics with Kafka, Spark & Airflow

ğŸš€ This project simulates a real-time clickstream analytics pipelineâ€”similar to those used in e-commerce platformsâ€”for capturing, processing, and visualizing user behavior.

---

## ğŸ§  Architecture Overview

**Pipeline Flow:**
Kafka â†’ Spark Structured Streaming â†’ Parquet/CSV â†’ Airflow DAGs â†’ Flask Dashboard + Tableau Reports

---

## âš™ï¸ Technologies Used
- **Apache Kafka** â€“ Click event ingestion
- **Apache Spark (PySpark)** â€“ Real-time stream processing with window aggregations
- **Apache Airflow** â€“ Job orchestration and automation
- **Flask + Plotly** â€“ Interactive real-time dashboard
- **Tableau** â€“ Visual analytics and trend exploration
- **Parquet/CSV** â€“ Data storage

---

## ğŸ” Key Features
- Kafka producer generates product click events
- Spark processes streaming data in 10-second time windows
- Airflow DAG schedules ETL workflows
- Final results saved in Parquet and CSV
- Flask + Plotly dashboard for live insights
- Tableau dashboard for post-analysis

---



## ğŸ“ **Repository Structure**
pgsql
Copy
Edit
â”œâ”€â”€ kafka_producer/        â†’ Python Kafka producer for click events  
â”œâ”€â”€ spark_streaming/       â†’ PySpark Structured Streaming script  
â”œâ”€â”€ airflow_dags/          â†’ Airflow DAG to orchestrate stream jobs  
â”œâ”€â”€ flask_dashboard/       â†’ Flask app to show real-time insights  
â”œâ”€â”€ data/                  â†’ Output data in Parquet/CSV  
â”œâ”€â”€ tableau_dashboard/     â†’ Tableau visuals/snapshots  
â”œâ”€â”€ requirements.txt       â†’ Python dependencies  
â””â”€â”€ README.md              â†’ Project overview  


## ğŸ“Š **Demo Dashboard**
![Flask With BS Dashboard1](https://github.com/user-attachments/assets/ee9e59e0-9ff6-44f7-9382-2a48ac5c8e51)
![Flask with BS Dashboard2](https://github.com/user-attachments/assets/5fa8d787-a387-4a21-931c-19420fe85b17)





## âœ… How to Run

Tested locally on **Windows + Ubuntu WSL**. You can simulate the full stack locally.
### Step-by-Step Instructions

#### 1ï¸âƒ£ Start Kafka Services (on Windows)
Start **Zookeeper** and **Kafka server**:

```bash
.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties
.\bin\windows\kafka-server-start.bat .\config\server.properties
2ï¸âƒ£ Run Kafka Producer (on Windows)
Sends simulated product click events to Kafka topic:

bash
Copy
Edit
python3 kafka_producer.py
3ï¸âƒ£ Run Spark Structured Streaming Consumer (on Ubuntu/WSL)
Consumes Kafka stream, aggregates data, and saves as Parquet and CSV:

bash
Copy
Edit
spark-submit spark_streaming_consumer.py
4ï¸âƒ£ Start Apache Airflow Scheduler (on Ubuntu/WSL)
Schedules or triggers the Spark job via Airflow DAG:

bash
Copy
Edit
source ~/airflow_env/bin/activate
airflow scheduler
5ï¸âƒ£ Launch Flask Dashboard (on Ubuntu/WSL)
Reads Parquet files and displays insights:

bash
Copy
Edit
python3 app.py
Then open your browser and go to:
http://localhost:5000



---



---

## ğŸ“ **License**
This project is licensed under the MIT License.

## ğŸ“¬ **Contact**
For any inquiries or suggestions, feel free to reach out:

Name: Swetha

Email: swethachowdhary33@gmail.com

GitHub: SWETHAY9

